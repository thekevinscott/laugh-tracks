{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "def installDeps():\n",
    "    !pip install numpy scipy\n",
    "    !pip install resampy tensorflow six\n",
    "    !pip install youtube_dl\n",
    "    !pip install ipywidgets\n",
    "    !pip install pydub\n",
    "    !pip install tqdm\n",
    "    !pip install ffmpeg-python\n",
    "    !apt-get install ffmpeg -y\n",
    "    #!wget https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "    #!wget https://storage.googleapis.com/audioset/vggish_pca_params.npz\n",
    "#installDeps()\n",
    "#!python vggish_train_demo.py --num_batches 50 --train_vggish=False --checkpoint './vggish_model.ckpt'\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from pydub import AudioSegment\n",
    "from audioModel import predict, train\n",
    "from audioInput import getLaughTracks, getNoise\n",
    "from audioUtils import readFolder\n",
    "\n",
    "def getModel(path):\n",
    "    files = readFolder('model/%s' % path)\n",
    "    if len(files) > 0:\n",
    "        return '%s/%s' % (path, files[0])\n",
    "    return None\n",
    "\n",
    "def trainAndSaveAndPredict(test_data, model, number_of_classes = 2, number_of_samples = 1, epochs = 5, getData = getLaughTracks, log = True):\n",
    "    model_name = '%s_%s' % (model, number_of_samples)\n",
    "    def curriedGetSamples(shuf):\n",
    "        return getData(number_of_samples = number_of_samples, shuf = shuf, log = log)\n",
    "    print('model_name', model_name)\n",
    "    preds = train(curriedGetSamples, number_of_classes, model_name = model_name, epochs = epochs)\n",
    "    \n",
    "    return predict(getModel('%s' % (model_name)), number_of_classes, test_data)\n",
    "\n",
    "def printResults(preds, expected = None): \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        print(preds)\n",
    "        print(sess.run(tf.argmax(input=preds, axis=1))) \n",
    "        print('expected results', expected)\n",
    "\n",
    "def trainForNoise(number_of_samples=5, epochs=5):\n",
    "    print('training on noise, sin, and constant waves')\n",
    "    (features, labels) = getNoise(shuf=False, number_of_samples = 2)\n",
    "    preds = trainAndSaveAndPredict(features, 'noise', number_of_classes = 3, number_of_samples = number_of_samples, epochs = epochs, getData = getNoise)\n",
    "    printResults(preds, [0, 0, 1, 1, 2, 2])\n",
    "    \n",
    "def trainForLaughter(number_of_samples=5, epochs=5):  \n",
    "    #print('training on laughter and not laughter')\n",
    "    (features, labels) = getLaughTracks(shuf=False, number_of_samples = 2, log=False)\n",
    "    preds = trainAndSaveAndPredict(features, 'audio', number_of_classes = 2, number_of_samples = number_of_samples, epochs = epochs, getData = getLaughTracks, log = False)\n",
    "    printResults(preds, [0, 0, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainForLaughter(number_of_samples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range (0, 30):\n",
    "#    number_of_samples = 1006 + (i * 1)\n",
    "#    trainForLaughter(number_of_samples=number_of_samples, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name audio_100\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Step 1: loss 0.738319\n",
      "saving model ./model/audio_100/1_20/model\n",
      "Step 2: loss 0.703907\n",
      "saving model ./model/audio_100/2_20/model\n",
      "Step 3: loss 0.680426\n",
      "saving model ./model/audio_100/3_20/model\n",
      "Step 4: loss 0.686973\n",
      "saving model ./model/audio_100/4_20/model\n",
      "Step 5: loss 0.671367\n",
      "saving model ./model/audio_100/5_20/model\n",
      "Step 6: loss 0.670204\n",
      "saving model ./model/audio_100/6_20/model\n",
      "Step 7: loss 0.656642\n",
      "saving model ./model/audio_100/7_20/model\n",
      "Step 8: loss 0.610177\n",
      "saving model ./model/audio_100/8_20/model\n",
      "Step 9: loss 0.637092\n",
      "saving model ./model/audio_100/9_20/model\n",
      "Step 10: loss 0.59811\n",
      "saving model ./model/audio_100/10_20/model\n",
      "Step 11: loss 0.596985\n",
      "saving model ./model/audio_100/11_20/model\n",
      "Step 12: loss 0.613042\n",
      "saving model ./model/audio_100/12_20/model\n",
      "Step 13: loss 0.590328\n",
      "saving model ./model/audio_100/13_20/model\n",
      "Step 14: loss 0.554993\n",
      "saving model ./model/audio_100/14_20/model\n",
      "Step 15: loss 0.490383\n",
      "saving model ./model/audio_100/15_20/model\n",
      "Step 16: loss 0.554873\n",
      "saving model ./model/audio_100/16_20/model\n",
      "Step 17: loss 0.56587\n",
      "saving model ./model/audio_100/17_20/model\n",
      "Step 18: loss 0.458461\n",
      "saving model ./model/audio_100/18_20/model\n",
      "Step 19: loss 0.523965\n",
      "saving model ./model/audio_100/19_20/model\n",
      "Step 20: loss 0.43\n",
      "saving model ./model/audio_100/20_20/model\n",
      "loading ./model/audio_100/20_20/model\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/audio_100/20_20/model\n",
      "[[0.43939427 0.5663858 ]\n",
      " [0.6855665  0.39353812]\n",
      " [0.45551136 0.51574916]\n",
      " [0.50967735 0.5104256 ]]\n",
      "[1 0 1 1]\n",
      "expected results [0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "trainForLaughter(number_of_samples=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name audio_201\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Step 1: loss 0.732376\n",
      "saving model ./model/audio_201/1_40/model\n",
      "Step 2: loss 0.633559\n",
      "saving model ./model/audio_201/2_40/model\n",
      "Step 3: loss 0.593709\n",
      "saving model ./model/audio_201/3_40/model\n",
      "Step 4: loss 0.572072\n",
      "saving model ./model/audio_201/4_40/model\n",
      "Step 5: loss 0.68271\n",
      "saving model ./model/audio_201/5_40/model\n",
      "Step 6: loss 0.595793\n",
      "saving model ./model/audio_201/6_40/model\n",
      "Step 7: loss 0.596696\n",
      "saving model ./model/audio_201/7_40/model\n",
      "Step 8: loss 0.556086\n",
      "saving model ./model/audio_201/8_40/model\n",
      "Step 9: loss 0.637458\n",
      "saving model ./model/audio_201/9_40/model\n",
      "Step 10: loss 0.590949\n",
      "saving model ./model/audio_201/10_40/model\n",
      "Step 11: loss 0.663197\n",
      "saving model ./model/audio_201/11_40/model\n",
      "Step 12: loss 0.565656\n",
      "saving model ./model/audio_201/12_40/model\n",
      "Step 13: loss 0.570229\n",
      "saving model ./model/audio_201/13_40/model\n",
      "Step 14: loss 0.843573\n",
      "saving model ./model/audio_201/14_40/model\n",
      "Step 15: loss 0.458405\n",
      "saving model ./model/audio_201/15_40/model\n",
      "Step 16: loss 0.577334\n",
      "saving model ./model/audio_201/16_40/model\n",
      "Step 17: loss 0.624169\n",
      "saving model ./model/audio_201/17_40/model\n",
      "Step 18: loss 0.491426\n",
      "saving model ./model/audio_201/18_40/model\n",
      "Step 19: loss 0.613651\n",
      "saving model ./model/audio_201/19_40/model\n",
      "Step 20: loss 0.480963\n",
      "saving model ./model/audio_201/20_40/model\n",
      "Step 21: loss 0.426142\n",
      "saving model ./model/audio_201/21_40/model\n",
      "Step 22: loss 0.445747\n",
      "saving model ./model/audio_201/22_40/model\n",
      "Step 23: loss 0.856125\n",
      "saving model ./model/audio_201/23_40/model\n",
      "Step 24: loss 0.353715\n",
      "saving model ./model/audio_201/24_40/model\n",
      "Step 25: loss 0.703046\n",
      "saving model ./model/audio_201/25_40/model\n",
      "Step 26: loss 0.602529\n",
      "saving model ./model/audio_201/26_40/model\n",
      "Step 27: loss 0.426252\n",
      "saving model ./model/audio_201/27_40/model\n",
      "Step 28: loss 0.45402\n",
      "saving model ./model/audio_201/28_40/model\n",
      "Step 29: loss 0.530153\n",
      "saving model ./model/audio_201/29_40/model\n",
      "Step 30: loss 0.577971\n",
      "saving model ./model/audio_201/30_40/model\n",
      "Step 31: loss 0.475823\n",
      "saving model ./model/audio_201/31_40/model\n",
      "Step 32: loss 0.468161\n",
      "saving model ./model/audio_201/32_40/model\n",
      "Step 33: loss 0.540089\n",
      "saving model ./model/audio_201/33_40/model\n",
      "Step 34: loss 0.351798\n",
      "saving model ./model/audio_201/34_40/model\n",
      "Step 35: loss 0.30563\n",
      "saving model ./model/audio_201/35_40/model\n",
      "Step 36: loss 0.472183\n",
      "saving model ./model/audio_201/36_40/model\n",
      "Step 37: loss 0.336864\n",
      "saving model ./model/audio_201/37_40/model\n",
      "Step 38: loss 0.398789\n",
      "saving model ./model/audio_201/38_40/model\n",
      "Step 39: loss 0.303349\n",
      "saving model ./model/audio_201/39_40/model\n",
      "Step 40: loss 0.37132\n",
      "saving model ./model/audio_201/40_40/model\n",
      "loading ./model/audio_201/40_40/model\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/audio_201/40_40/model\n",
      "[[0.4287952  0.7107146 ]\n",
      " [0.7784004  0.31745234]\n",
      " [0.8533493  0.19718693]\n",
      " [0.39970633 0.5429855 ]]\n",
      "[1 0 0 1]\n",
      "expected results [0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "trainForLaughter(number_of_samples=201, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./model/audio_201/40_40/model\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/audio_201/40_40/model\n",
      "[[0.4287952  0.7107146 ]\n",
      " [0.7784004  0.31745234]\n",
      " [0.7917802  0.2827743 ]\n",
      " [0.80329025 0.26296672]\n",
      " [0.7596626  0.34796327]\n",
      " [0.8183522  0.26691192]\n",
      " [0.9127813  0.16164927]\n",
      " [0.9300833  0.13568938]\n",
      " [0.87669116 0.19700894]\n",
      " [0.90420705 0.16216436]\n",
      " [0.8533493  0.19718693]\n",
      " [0.39970633 0.5429855 ]\n",
      " [0.3739197  0.6703356 ]\n",
      " [0.179904   0.7579397 ]\n",
      " [0.28311822 0.7237356 ]\n",
      " [0.1798182  0.8356218 ]\n",
      " [0.16748893 0.7691494 ]\n",
      " [0.17559975 0.72274864]\n",
      " [0.3164792  0.6167977 ]\n",
      " [0.24945058 0.663799  ]]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "expected results None\n"
     ]
    }
   ],
   "source": [
    "(features, labels) = getLaughTracks(shuf=False, number_of_samples = 10, log=False)\n",
    "printResults(predict(getModel('%s' % ('audio_201')), 2, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
