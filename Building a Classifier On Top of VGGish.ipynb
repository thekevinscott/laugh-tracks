{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos\n",
    "\n",
    "* Separate out prediction, loading the model\n",
    "* Confirm audio samples are being loaded in a way that is extensible\n",
    "* Train with all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installDeps():\n",
    "    !pip install numpy scipy\n",
    "    !pip install resampy tensorflow six\n",
    "    !pip install youtube_dl\n",
    "    !pip install ipywidgets\n",
    "    !pip install pydub\n",
    "    !pip install tqdm\n",
    "    !pip install ffmpeg-python\n",
    "    !apt-get install ffmpeg\n",
    "#!python vggish_train_demo.py --num_batches 50 --train_vggish=False --checkpoint './vggish_model.ckpt'\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from pydub import AudioSegment\n",
    "from audioUtils import readFolder\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNoise(shuf = True, number_of_samples = 1):\n",
    "    \"\"\"Returns a shuffled batch of examples of all audio classes.\n",
    "\n",
    "    Note that this is just a toy function because this is a simple demo intended\n",
    "    to illustrate how the training code might work.\n",
    "\n",
    "    Returns:\n",
    "    a tuple (features, labels) where features is a NumPy array of shape\n",
    "    [batch_size, num_frames, num_bands] where the batch_size is variable and\n",
    "    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n",
    "    suitable for feeding VGGish, while labels is a NumPy array of shape\n",
    "    [batch_size, num_classes] where each row is a multi-hot label vector that\n",
    "    provides the labels for corresponding rows in features.\n",
    "    \"\"\"\n",
    "    # Make a waveform for each class.\n",
    "    num_seconds = number_of_samples\n",
    "    sr = 44100  # Sampling rate.\n",
    "    t = np.linspace(0, num_seconds, int(num_seconds * sr))  # Time axis.\n",
    "    # Random sine wave.\n",
    "    freq = np.random.uniform(100, 1000)\n",
    "    sine = np.sin(2 * np.pi * freq * t)\n",
    "    # Random constant signal.\n",
    "    magnitude = np.random.uniform(-1, 1)\n",
    "    const = magnitude * t\n",
    "    # White noise.\n",
    "    noise = np.random.normal(-1, 1, size=t.shape)\n",
    "\n",
    "    # Make examples of each signal and corresponding labels.\n",
    "    # Sine is class index 0, Const class index 1, Noise class index 2.\n",
    "    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n",
    "    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n",
    "    const_examples = vggish_input.waveform_to_examples(const, sr)\n",
    "    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n",
    "    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n",
    "    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n",
    "\n",
    "    # Shuffle (example, label) pairs across all classes.\n",
    "    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n",
    "    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "def getFilePathsForClass(c):\n",
    "    dirs = readFolder('samples/%s' % (c))\n",
    "    collected_files = []\n",
    "    for d in dirs[:1]:\n",
    "        files = readFolder('samples/%s/%s/out' % (c, d))\n",
    "\n",
    "        for file in files:\n",
    "            collected_files.append('samples/%s/%s/out/%s' % (c, d, file))\n",
    "    return collected_files\n",
    "            \n",
    "def getSampleForFile(file):\n",
    "    return AudioSegment.from_file(file).get_array_of_samples()\n",
    "\n",
    "# accepts a numpy array representing a single audio file, or multiple files concat'ed together\n",
    "def getFileAsVggishInput(sample):\n",
    "    return vggish_input.waveform_to_examples(sample, 44100)\n",
    "\n",
    "# append every audio file into one enormous massive audio file\n",
    "def getSamplesForFiles(files, number_of_samples):\n",
    "    sample = np.array([])\n",
    "    \n",
    "    for file in files:\n",
    "        audio = getSampleForFile(file)\n",
    "        sample = np.append(sample, audio)\n",
    "        \n",
    "    vggishInput = getFileAsVggishInput(sample)[0:number_of_samples]\n",
    "    return vggishInput\n",
    "\n",
    "def getData(files, number_of_samples, arr):\n",
    "    examples = getSamplesForFiles(files, number_of_samples)\n",
    "    labels = np.array([arr] * examples.shape[0])\n",
    "    \n",
    "    return (examples, labels)\n",
    "\n",
    "def getOneHot(class_num, idx):\n",
    "    arr = np.zeros(class_num)\n",
    "    arr[idx] = 1\n",
    "    return arr\n",
    "\n",
    "def getSamples(classes, shuf = True, number_of_samples = None):\n",
    "    exes = []\n",
    "    whys = []\n",
    "    #print('collecting samples')\n",
    "    for idx, cls in enumerate(classes):\n",
    "        files = getFilePathsForClass(cls)\n",
    "        x, y = getData(files, number_of_samples, getOneHot(len(classes), idx))\n",
    "        exes.append(x)\n",
    "        whys.append(y)\n",
    "    \n",
    "    all_examples = np.concatenate(exes)\n",
    "    all_labels = np.concatenate(whys)\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "def loadVGGish(sess, number_of_classes):\n",
    "    embeddings = vggish_slim.define_vggish_slim(True) # Do we train VGG-ish?\n",
    "\n",
    "    # Define a shallow classification model and associated training ops on top\n",
    "    # of VGGish.\n",
    "    with tf.variable_scope('mymodel'):\n",
    "        # Add a fully connected layer with 100 units.\n",
    "        num_units = 100\n",
    "        fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "        # Add a classifier layer at the end, consisting of parallel logistic\n",
    "        # classifiers, one per class. This allows for multi-class tasks.\n",
    "        logits = slim.fully_connected(\n",
    "          fc, number_of_classes, activation_fn=None, scope='logits')\n",
    "        pred = tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "        # Add training ops.\n",
    "        with tf.variable_scope('train'):\n",
    "            global_step = tf.Variable(\n",
    "                0, name='global_step', trainable=False,\n",
    "                collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "        labels = tf.placeholder(\n",
    "            tf.float32, shape=(None, number_of_classes), name='labels')\n",
    "\n",
    "        # Cross-entropy label loss.\n",
    "        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels, name='xent')\n",
    "        loss = tf.reduce_mean(xent, name='loss_op')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=vggish_params.LEARNING_RATE,\n",
    "            epsilon=vggish_params.ADAM_EPSILON)\n",
    "        optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "    # Initialize all variables in the model, and then load the pre-trained\n",
    "    # VGGish checkpoint.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt') \n",
    "    return logits, pred\n",
    "    \n",
    "    \n",
    "def train(get_examples, number_of_classes, model_name = 'model', epochs = 50):\n",
    "    model_name_to_save = './model/%s' % (model_name)    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        # Define VGGish.\n",
    "        logits, pred = loadVGGish(sess, number_of_classes)\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        #for op in tf.get_default_graph().get_operations():\n",
    "            #print(str(op.name))\n",
    "\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/labels:0')\n",
    "        #labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')    \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/loss_op:0')\n",
    "        train_op = sess.graph.get_operation_by_name('mymodel/train_op')\n",
    "\n",
    "        # The training loop.\n",
    "        for _ in range(epochs):\n",
    "            (features, labels) = get_examples(shuf=True)\n",
    "            [num_steps, loss, _] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, train_op],\n",
    "                feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "            print('Step %d: loss %g' % (num_steps, loss))\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, model_name_to_save)            \n",
    "\n",
    "def predict(model_name, number_of_classes, features):\n",
    "    model_name_to_load = './model/%s' % (model_name)   \n",
    "\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        logits, pred = loadVGGish(sess, number_of_classes)\n",
    "        saver = tf.train.Saver()        \n",
    "        saver.restore(sess, model_name_to_load)  \n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        prediction=tf.argmax(logits,1)\n",
    "        embedding_batch = sess.run(pred, feed_dict={features_tensor: features})\n",
    "        return embedding_batch \n",
    "\n",
    "def getLaughTracks(number_of_samples = 1, shuf = True, use_cache = True):\n",
    "    features_name = 'checkpoints/features_%s.npy' % (number_of_samples)\n",
    "    labels_name = 'checkpoints/labels_%s.npy' % (number_of_samples)\n",
    "    \n",
    "    if use_cache and os.path.isfile(features_name) and os.path.isfile(labels_name):\n",
    "        #print('using cache for laugh tracks')\n",
    "        features = np.load(features_name)\n",
    "        labels = np.load(labels_name)        \n",
    "    else:\n",
    "        #print('not using cache for laugh tracks')\n",
    "        (features, labels) = getSamples(['laughter', 'notlaughter'], shuf = False, number_of_samples = number_of_samples)\n",
    "        np.save('checkpoints/features_%s.npy' % (number_of_samples), features)\n",
    "        np.save('checkpoints/labels_%s.npy' % (number_of_samples), labels)\n",
    "\n",
    "    labeled_examples = list(zip(features, labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "def printResults(preds, expected = None): \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        print(preds)\n",
    "        print(sess.run(tf.argmax(input=preds, axis=1))) \n",
    "        print('expected results', expected)\n",
    "\n",
    "\n",
    "def trainAndSaveAndPredict(test_data, number_of_classes, number_of_samples = 1, epochs = 5, getData = getLaughTracks):\n",
    "    def curriedGetSamples(shuf):\n",
    "        return getData(number_of_samples = number_of_samples, shuf = shuf)\n",
    "    model_name = 'model_%s' % (number_of_samples)\n",
    "    preds = train(curriedGetSamples, number_of_classes, model_name = model_name, epochs = epochs)\n",
    "    \n",
    "\n",
    "    return predict(model_name, number_of_classes, test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on laughter and not laughter\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Tensor(\"mymodel/logits/BiasAdd:0\", shape=(?, 2), dtype=float32) Tensor(\"mymodel/prediction:0\", shape=(?, 2), dtype=float32)\n",
      "Step 1: loss 0.697954\n",
      "Step 2: loss 0.694393\n",
      "Step 3: loss 0.691415\n",
      "Step 4: loss 0.688246\n",
      "Step 5: loss 0.687291\n",
      "Step 6: loss 0.685232\n",
      "Step 7: loss 0.68257\n",
      "Step 8: loss 0.682246\n",
      "Step 9: loss 0.679612\n",
      "Step 10: loss 0.675931\n",
      "Step 11: loss 0.673282\n",
      "Step 12: loss 0.670613\n",
      "Step 13: loss 0.66862\n",
      "Step 14: loss 0.663782\n",
      "Step 15: loss 0.661149\n",
      "Step 16: loss 0.65827\n",
      "Step 17: loss 0.654163\n",
      "Step 18: loss 0.65052\n",
      "Step 19: loss 0.646273\n",
      "Step 20: loss 0.642218\n",
      "Step 21: loss 0.636905\n",
      "Step 22: loss 0.633898\n",
      "Step 23: loss 0.626451\n",
      "Step 24: loss 0.621377\n",
      "Step 25: loss 0.614254\n",
      "Step 26: loss 0.608983\n",
      "Step 27: loss 0.601575\n",
      "Step 28: loss 0.595538\n",
      "Step 29: loss 0.587712\n",
      "Step 30: loss 0.582941\n",
      "Step 31: loss 0.572704\n",
      "Step 32: loss 0.566669\n",
      "Step 33: loss 0.558553\n",
      "Step 34: loss 0.549212\n",
      "Step 35: loss 0.539809\n",
      "Step 36: loss 0.531339\n",
      "Step 37: loss 0.519358\n",
      "Step 38: loss 0.509767\n",
      "Step 39: loss 0.498207\n",
      "Step 40: loss 0.488969\n",
      "Step 41: loss 0.479485\n",
      "Step 42: loss 0.468339\n",
      "Step 43: loss 0.454009\n",
      "Step 44: loss 0.440365\n",
      "Step 45: loss 0.426802\n",
      "Step 46: loss 0.415113\n",
      "Step 47: loss 0.409219\n",
      "Step 48: loss 0.405193\n",
      "Step 49: loss 0.385647\n",
      "Step 50: loss 0.367286\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Tensor(\"mymodel/logits/BiasAdd:0\", shape=(?, 2), dtype=float32) Tensor(\"mymodel/prediction:0\", shape=(?, 2), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_10\n",
      "(4, 96, 64)\n",
      "[[0.7730319  0.33012906]\n",
      " [0.6821969  0.40904263]\n",
      " [0.02258665 0.9689251 ]\n",
      " [0.25739044 0.6852253 ]]\n",
      "[0 0 1 1]\n",
      "expected results [0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "number_of_samples = 10\n",
    "epochs = 50\n",
    "#print('training on noise, sin, and constant waves')\n",
    "#(features, labels) = getNoise(shuf=False, number_of_samples = 2)\n",
    "#preds = trainAndSaveAndPredict(features, number_of_samples = number_of_samples, epochs = epochs, number_of_classes = 3, getData = getNoise)\n",
    "#printResults(preds, [0, 0, 1, 1, 2, 2])\n",
    "print('training on laughter and not laughter')\n",
    "(features, labels) = getLaughTracks(shuf=False, number_of_samples = 2)\n",
    "preds = trainAndSaveAndPredict(features, number_of_samples = number_of_samples, epochs = epochs, number_of_classes = 2, getData = getLaughTracks)\n",
    "printResults(preds, [0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 96, 64)\n",
      "(3, 3)\n",
      "(2, 96, 64)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trainAndSaveAndPredict2(number_of_samples = 3, epochs = 5)\n",
    "(features, labels) = getNoise(shuf = False, number_of_samples = 1)\n",
    "noise_f = features\n",
    "noise_l = labels\n",
    "(features, labels) = getSamples(['laughter', 'notlaughter'], shuf = False, number_of_samples = 1)\n",
    "yt_f = features\n",
    "yt_l = labels\n",
    "\n",
    "print(np.array(noise_f).shape)\n",
    "print(np.array(noise_l).shape)\n",
    "print(np.array(yt_f).shape)\n",
    "print(np.array(yt_l).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORKING IMPLEMENTATION OF TRAIN\n",
    "def train(get_examples, number_of_classes, model_name = 'model', epochs = 50):\n",
    "    model_name_to_save = './model/%s' % (model_name)    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        pred = None\n",
    "        # Define VGGish.\n",
    "        embeddings = vggish_slim.define_vggish_slim(True) # Do we train VGG-ish?\n",
    "\n",
    "        # Define a shallow classification model and associated training ops on top\n",
    "        # of VGGish.\n",
    "        with tf.variable_scope('mymodel'):\n",
    "            # Add a fully connected layer with 100 units.\n",
    "            num_units = 100\n",
    "            fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "            # classifiers, one per class. This allows for multi-class tasks.\n",
    "            logits = slim.fully_connected(\n",
    "              fc, number_of_classes, activation_fn=None, scope='logits')\n",
    "            pred = tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "            # Add training ops.\n",
    "            with tf.variable_scope('train'):\n",
    "                global_step = tf.Variable(\n",
    "                    0, name='global_step', trainable=False,\n",
    "                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                 tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "            # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "            # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "            labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, number_of_classes), name='labels')\n",
    "\n",
    "            # Cross-entropy label loss.\n",
    "            xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=logits, labels=labels, name='xent')\n",
    "            loss = tf.reduce_mean(xent, name='loss_op')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=vggish_params.LEARNING_RATE,\n",
    "                epsilon=vggish_params.ADAM_EPSILON)\n",
    "            optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "        # Initialize all variables in the model, and then load the pre-trained\n",
    "        # VGGish checkpoint.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        #for op in tf.get_default_graph().get_operations():\n",
    "            #print(str(op.name))\n",
    "\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/labels:0')\n",
    "        #labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')    \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/loss_op:0')\n",
    "        train_op = sess.graph.get_operation_by_name('mymodel/train_op')\n",
    "\n",
    "        # The training loop.\n",
    "        for _ in range(epochs):\n",
    "            (features, labels) = get_examples(shuf=True)\n",
    "            [num_steps, loss, _] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, train_op],\n",
    "                feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "            print('Step %d: loss %g' % (num_steps, loss))\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, model_name_to_save)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # FIGURE OUT HOW TO LOAD THE SAVED MODEL HERE\n",
    "\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        prediction=tf.argmax(logits,1)\n",
    "        (features, labels) = get_examples(shuf=False)\n",
    "        embedding_batch = sess.run(pred, feed_dict={features_tensor: features})\n",
    "        return embedding_batch "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
