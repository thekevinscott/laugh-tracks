{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos\n",
    "\n",
    "* Separate out prediction, loading the model\n",
    "* Confirm audio samples are being loaded in a way that is extensible\n",
    "* Train with all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installDeps():\n",
    "    !pip install numpy scipy\n",
    "    !pip install resampy tensorflow six\n",
    "    !pip install youtube_dl\n",
    "    !pip install ipywidgets\n",
    "    !pip install pydub\n",
    "    !pip install tqdm\n",
    "    !pip install ffmpeg-python\n",
    "    !apt-get install ffmpeg\n",
    "#!python vggish_train_demo.py --num_batches 50 --train_vggish=False --checkpoint './vggish_model.ckpt'\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from pydub import AudioSegment\n",
    "from audioUtils import readFolder\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNoise(shuf = True, number_of_samples = 1):\n",
    "    \"\"\"Returns a shuffled batch of examples of all audio classes.\n",
    "\n",
    "    Note that this is just a toy function because this is a simple demo intended\n",
    "    to illustrate how the training code might work.\n",
    "\n",
    "    Returns:\n",
    "    a tuple (features, labels) where features is a NumPy array of shape\n",
    "    [batch_size, num_frames, num_bands] where the batch_size is variable and\n",
    "    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n",
    "    suitable for feeding VGGish, while labels is a NumPy array of shape\n",
    "    [batch_size, num_classes] where each row is a multi-hot label vector that\n",
    "    provides the labels for corresponding rows in features.\n",
    "    \"\"\"\n",
    "    # Make a waveform for each class.\n",
    "    num_seconds = number_of_samples\n",
    "    sr = 44100  # Sampling rate.\n",
    "    t = np.linspace(0, num_seconds, int(num_seconds * sr))  # Time axis.\n",
    "    # Random sine wave.\n",
    "    freq = np.random.uniform(100, 1000)\n",
    "    sine = np.sin(2 * np.pi * freq * t)\n",
    "    # Random constant signal.\n",
    "    magnitude = np.random.uniform(-1, 1)\n",
    "    const = magnitude * t\n",
    "    # White noise.\n",
    "    noise = np.random.normal(-1, 1, size=t.shape)\n",
    "\n",
    "    # Make examples of each signal and corresponding labels.\n",
    "    # Sine is class index 0, Const class index 1, Noise class index 2.\n",
    "    sine_examples = vggish_input.waveform_to_examples(sine, sr)\n",
    "    sine_labels = np.array([[1, 0, 0]] * sine_examples.shape[0])\n",
    "    const_examples = vggish_input.waveform_to_examples(const, sr)\n",
    "    const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n",
    "    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n",
    "    noise_labels = np.array([[0, 0, 1]] * noise_examples.shape[0])\n",
    "\n",
    "    # Shuffle (example, label) pairs across all classes.\n",
    "    all_examples = np.concatenate((sine_examples, const_examples, noise_examples))\n",
    "    all_labels = np.concatenate((sine_labels, const_labels, noise_labels))\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "def getFilePathsForClass(c):\n",
    "    dirs = readFolder('samples/%s' % (c))\n",
    "    collected_files = []\n",
    "    for d in dirs[:1]:\n",
    "        files = readFolder('samples/%s/%s/out' % (c, d))\n",
    "\n",
    "        for file in files:\n",
    "            collected_files.append('samples/%s/%s/out/%s' % (c, d, file))\n",
    "    return collected_files\n",
    "            \n",
    "def getSampleForFile(file):\n",
    "    return AudioSegment.from_file(file).get_array_of_samples()\n",
    "\n",
    "# accepts a numpy array representing a single audio file, or multiple files concat'ed together\n",
    "def getFileAsVggishInput(sample):\n",
    "    return vggish_input.waveform_to_examples(sample, 44100)\n",
    "\n",
    "# append every audio file into one enormous massive audio file\n",
    "def getSamplesForFiles(files, number_of_samples):\n",
    "    sample = np.array([])\n",
    "    \n",
    "    for file in files:\n",
    "        audio = getSampleForFile(file)\n",
    "        sample = np.append(sample, audio)\n",
    "        \n",
    "    vggishInput = getFileAsVggishInput(sample)[0:number_of_samples]\n",
    "    return vggishInput\n",
    "\n",
    "def getData(files, number_of_samples, arr):\n",
    "    examples = getSamplesForFiles(files, number_of_samples)\n",
    "    labels = np.array([arr] * examples.shape[0])\n",
    "    \n",
    "    return (examples, labels)\n",
    "\n",
    "def getOneHot(class_num, idx):\n",
    "    arr = np.zeros(class_num)\n",
    "    arr[idx] = 1\n",
    "    return arr\n",
    "\n",
    "def getSamples(classes, shuf = True, number_of_samples = None):\n",
    "    exes = []\n",
    "    whys = []\n",
    "    #print('collecting samples')\n",
    "    for idx, cls in enumerate(classes):\n",
    "        files = getFilePathsForClass(cls)\n",
    "        x, y = getData(files, number_of_samples, getOneHot(len(classes), idx))\n",
    "        exes.append(x)\n",
    "        whys.append(y)\n",
    "    \n",
    "    all_examples = np.concatenate(exes)\n",
    "    all_labels = np.concatenate(whys)\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "def loadVGGish(sess, number_of_classes):\n",
    "    embeddings = vggish_slim.define_vggish_slim(True) # Do we train VGG-ish?\n",
    "\n",
    "    # Define a shallow classification model and associated training ops on top\n",
    "    # of VGGish.\n",
    "    with tf.variable_scope('mymodel'):\n",
    "        # Add a fully connected layer with 100 units.\n",
    "        num_units = 100\n",
    "        fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "        # Add a classifier layer at the end, consisting of parallel logistic\n",
    "        # classifiers, one per class. This allows for multi-class tasks.\n",
    "        logits = slim.fully_connected(\n",
    "          fc, number_of_classes, activation_fn=None, scope='logits')\n",
    "        pred = tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "        # Add training ops.\n",
    "        with tf.variable_scope('train'):\n",
    "            global_step = tf.Variable(\n",
    "                0, name='global_step', trainable=False,\n",
    "                collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "        # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "        # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "        labels = tf.placeholder(\n",
    "            tf.float32, shape=(None, number_of_classes), name='labels')\n",
    "\n",
    "        # Cross-entropy label loss.\n",
    "        xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels, name='xent')\n",
    "        loss = tf.reduce_mean(xent, name='loss_op')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=vggish_params.LEARNING_RATE,\n",
    "            epsilon=vggish_params.ADAM_EPSILON)\n",
    "        optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "    # Initialize all variables in the model, and then load the pre-trained\n",
    "    # VGGish checkpoint.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "    print(logits, pred)    \n",
    "    return logits, pred\n",
    "    \n",
    "    \n",
    "def train(get_examples, number_of_classes, model_name = 'model', epochs = 50):\n",
    "    model_name_to_save = './model/%s' % (model_name)    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        # Define VGGish.\n",
    "        logits, pred = loadVGGish(sess, number_of_classes)\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        #for op in tf.get_default_graph().get_operations():\n",
    "            #print(str(op.name))\n",
    "\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/labels:0')\n",
    "        #labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')    \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/loss_op:0')\n",
    "        train_op = sess.graph.get_operation_by_name('mymodel/train_op')\n",
    "\n",
    "        # The training loop.\n",
    "        for _ in range(epochs):\n",
    "            (features, labels) = get_examples(shuf=True)\n",
    "            [num_steps, loss, _] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, train_op],\n",
    "                feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "            print('Step %d: loss %g' % (num_steps, loss))\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, model_name_to_save)            \n",
    "\n",
    "def predict(model_name, number_of_classes, features, get_examples):\n",
    "    model_name_to_load = './model/%s' % (model_name)   \n",
    "\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        logits, pred = loadVGGish(sess, number_of_classes)\n",
    "        (features, labels) = get_examples(shuf=False)\n",
    "        saver = tf.train.Saver()        \n",
    "        saver.restore(sess, model_name_to_load)  \n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        prediction=tf.argmax(logits,1)\n",
    "        print(np.array(features).shape)\n",
    "        embedding_batch = sess.run(pred, feed_dict={features_tensor: features})\n",
    "        return embedding_batch \n",
    "\n",
    "def getLaughTracks(number_of_samples = 1, shuf = True, use_cache = True):\n",
    "    features_name = 'checkpoints/features_%s.npy' % (number_of_samples)\n",
    "    labels_name = 'checkpoints/labels_%s.npy' % (number_of_samples)\n",
    "    \n",
    "    if use_cache and os.path.isfile(features_name) and os.path.isfile(labels_name):\n",
    "        #print('using cache for laugh tracks')\n",
    "        features = np.load(features_name)\n",
    "        labels = np.load(labels_name)        \n",
    "    else:\n",
    "        #print('not using cache for laugh tracks')\n",
    "        (features, labels) = getSamples(['laughter', 'notlaughter'], shuf = False, number_of_samples = number_of_samples)\n",
    "        np.save('checkpoints/features_%s.npy' % (number_of_samples), features)\n",
    "        np.save('checkpoints/labels_%s.npy' % (number_of_samples), labels)\n",
    "\n",
    "    labeled_examples = list(zip(features, labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainAndSaveAndPredict(number_of_classes, number_of_samples = 1, epochs = 5, getData = getLaughTracks):\n",
    "    def curriedGetSamples(shuf):\n",
    "        return getData(number_of_samples = number_of_samples, shuf = shuf)\n",
    "    model_name = 'model_%s' % (number_of_samples)\n",
    "    preds = train(curriedGetSamples, number_of_classes, model_name = model_name, epochs = epochs)\n",
    "    \n",
    "    (features, labels) = getData(shuf=False)\n",
    "    print(np.array(features).shape)    \n",
    "    preds = predict(model_name, number_of_classes, features, getData)\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        print(preds)\n",
    "        print(sess.run(tf.argmax(input=preds, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on noise, sin, and constant waves\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Tensor(\"mymodel/logits/BiasAdd:0\", shape=(?, 3), dtype=float32) Tensor(\"mymodel/prediction:0\", shape=(?, 3), dtype=float32)\n",
      "Step 1: loss 0.779227\n",
      "(2, 3)\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Tensor(\"mymodel/logits/BiasAdd:0\", shape=(?, 3), dtype=float32) Tensor(\"mymodel/prediction:0\", shape=(?, 3), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model/model_1\n",
      "(3, 96, 64)\n",
      "[[0.48206213 0.5950609  0.54132783]\n",
      " [0.55020595 0.45138192 0.5996106 ]\n",
      " [0.5323726  0.41639867 0.45353687]]\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "number_of_samples = 1\n",
    "epochs = 1\n",
    "print('training on noise, sin, and constant waves')\n",
    "trainAndSaveAndPredict(number_of_samples = number_of_samples, epochs = epochs, number_of_classes = 3, getData = getNoise)\n",
    "#print('training on laughter and not laughter')\n",
    "#trainAndSaveAndPredict(number_of_samples = number_of_samples, epochs = epochs, number_of_classes = 2, getData = getLaughTracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 96, 64)\n",
      "(3, 3)\n",
      "(2, 96, 64)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trainAndSaveAndPredict2(number_of_samples = 3, epochs = 5)\n",
    "(features, labels) = getNoise(shuf = False, number_of_samples = 1)\n",
    "noise_f = features\n",
    "noise_l = labels\n",
    "(features, labels) = getSamples(['laughter', 'notlaughter'], shuf = False, number_of_samples = 1)\n",
    "yt_f = features\n",
    "yt_l = labels\n",
    "\n",
    "print(np.array(noise_f).shape)\n",
    "print(np.array(noise_l).shape)\n",
    "print(np.array(yt_f).shape)\n",
    "print(np.array(yt_l).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORKING IMPLEMENTATION OF TRAIN\n",
    "def train(get_examples, number_of_classes, model_name = 'model', epochs = 50):\n",
    "    model_name_to_save = './model/%s' % (model_name)    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        pred = None\n",
    "        # Define VGGish.\n",
    "        embeddings = vggish_slim.define_vggish_slim(True) # Do we train VGG-ish?\n",
    "\n",
    "        # Define a shallow classification model and associated training ops on top\n",
    "        # of VGGish.\n",
    "        with tf.variable_scope('mymodel'):\n",
    "            # Add a fully connected layer with 100 units.\n",
    "            num_units = 100\n",
    "            fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "            # classifiers, one per class. This allows for multi-class tasks.\n",
    "            logits = slim.fully_connected(\n",
    "              fc, number_of_classes, activation_fn=None, scope='logits')\n",
    "            pred = tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "            # Add training ops.\n",
    "            with tf.variable_scope('train'):\n",
    "                global_step = tf.Variable(\n",
    "                    0, name='global_step', trainable=False,\n",
    "                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                 tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "            # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "            # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "            labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, number_of_classes), name='labels')\n",
    "\n",
    "            # Cross-entropy label loss.\n",
    "            xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=logits, labels=labels, name='xent')\n",
    "            loss = tf.reduce_mean(xent, name='loss_op')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=vggish_params.LEARNING_RATE,\n",
    "                epsilon=vggish_params.ADAM_EPSILON)\n",
    "            optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "        # Initialize all variables in the model, and then load the pre-trained\n",
    "        # VGGish checkpoint.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        #for op in tf.get_default_graph().get_operations():\n",
    "            #print(str(op.name))\n",
    "\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/labels:0')\n",
    "        #labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')    \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/loss_op:0')\n",
    "        train_op = sess.graph.get_operation_by_name('mymodel/train_op')\n",
    "\n",
    "        # The training loop.\n",
    "        for _ in range(epochs):\n",
    "            (features, labels) = get_examples(shuf=True)\n",
    "            [num_steps, loss, _] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, train_op],\n",
    "                feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "            print('Step %d: loss %g' % (num_steps, loss))\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, model_name_to_save)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # FIGURE OUT HOW TO LOAD THE SAVED MODEL HERE\n",
    "\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        prediction=tf.argmax(logits,1)\n",
    "        (features, labels) = get_examples(shuf=False)\n",
    "        embedding_batch = sess.run(pred, feed_dict={features_tensor: features})\n",
    "        return embedding_batch "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
