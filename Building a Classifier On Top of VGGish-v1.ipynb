{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (1.15.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.5/dist-packages (1.1.0)\n",
      "Requirement already satisfied: resampy in /usr/local/lib/python3.5/dist-packages (0.2.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.5/dist-packages (1.10.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.5/dist-packages (from resampy) (0.39.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.5/dist-packages (from resampy) (1.15.1)\n",
      "Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.5/dist-packages (from resampy) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: llvmlite>=0.24.0dev0 in /usr/local/lib/python3.5/dist-packages (from numba>=0.32->resampy) (0.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.5/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: youtube_dl in /usr/local/lib/python3.5/dist-packages (2018.9.18)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.5/dist-packages (7.4.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.5/dist-packages (from ipywidgets) (4.3.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.5/dist-packages (from ipywidgets) (3.4.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.5/dist-packages (from ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.5/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.2.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.3.1->ipywidgets) (4.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.3.1->ipywidgets) (1.11.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.5/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.5/dist-packages (from widgetsnbextension~=3.4.0->ipywidgets) (5.6.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (39.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.0.15)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.7.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.5/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (17.1.2)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.5/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.3.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.5/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.8.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.5/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (2.10)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.5/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.3.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.5/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (2.1.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.5/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.3.1)\n",
      "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /usr/local/lib/python3.5/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.0.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.5/dist-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.5.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /usr/local/lib/python3.5/dist-packages (0.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages (4.26.0)\n",
      "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.5/dist-packages (0.1.16)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.5/dist-packages (from ffmpeg-python) (0.16.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:2.8.15-0ubuntu0.16.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "/bin/sh: 1: wget: not found\n",
      "/bin/sh: 1: wget: not found\n"
     ]
    }
   ],
   "source": [
    "def installDeps():\n",
    "    !pip install numpy scipy\n",
    "    !pip install resampy tensorflow six\n",
    "    !pip install youtube_dl\n",
    "    !pip install ipywidgets\n",
    "    !pip install pydub\n",
    "    !pip install tqdm\n",
    "    !pip install ffmpeg-python\n",
    "    !apt-get install ffmpeg\n",
    "    \n",
    "#installDeps()\n",
    "#!wget https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "#!wget https://storage.googleapis.com/audioset/vggish_pca_params.npz\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_slim\n",
    "from pydub import AudioSegment\n",
    "from audioUtils import readFolder\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting laughter\n",
      "building data for files1\n",
      "collecting not laughter\n",
      "collecting notlaughter\n",
      "building data for files2\n",
      "244\n"
     ]
    }
   ],
   "source": [
    "def collectFilesForClass(c, max):\n",
    "    dirs = readFolder('samples/%s' % (c))\n",
    "    collected_files = []\n",
    "    for d in dirs[:max]:\n",
    "        files = readFolder('samples/%s/%s/out' % (c, d))\n",
    "\n",
    "        for file in files:\n",
    "            collected_files.append('samples/%s/%s/out/%s' % (c, d, file))\n",
    "    return collected_files\n",
    "            \n",
    "def getSampleForFile(file):\n",
    "    return AudioSegment.from_file(file).get_array_of_samples()\n",
    "\n",
    "def getSamplesForFiles(files):\n",
    "    sample = np.array([])\n",
    "    \n",
    "    sr = 44100  # Sampling rate.\n",
    "    for file in files:\n",
    "        \n",
    "        sample = np.append(sample, getSampleForFile(file))\n",
    "    return vggish_input.waveform_to_examples(sample, sr)   \n",
    "\n",
    "def getData(files, arr):\n",
    "    examples = getSamplesForFiles(files)\n",
    "    labels = np.array([arr] * examples.shape[0])\n",
    "    \n",
    "    return (examples, labels)\n",
    "\n",
    "def get_samples(shuf = True, num = None):\n",
    "    #num_seconds = 5 * (len(files) * 2)\n",
    "    #t = np.linspace(0, num_seconds, int(num_seconds * sr))  # Time axis.\n",
    "    \n",
    "    print('collecting laughter')\n",
    "    files1 = collectFilesForClass('laughter', num)\n",
    "    print('building data for files1')    \n",
    "    x1, y1 = getData(files1, [1, 0])\n",
    "    print('collecting not laughter')\n",
    "\n",
    "    print('collecting notlaughter')\n",
    "    files2 = collectFilesForClass('notlaughter', num)\n",
    "    print('building data for files2')    \n",
    "    x2, y2 = getData(files2, [0, 1])\n",
    "    \n",
    "    all_examples = np.concatenate((x1, x2))\n",
    "    all_labels = np.concatenate((y1, y2))\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "num = 1\n",
    "(features, labels) = get_samples(shuf = False, num = num)\n",
    "print(len(features))\n",
    "np.save('checkpoints/features_%s.npy' % (num), features)\n",
    "np.save('checkpoints/labels_%s.npy' % (num), labels)\n",
    "# returns 3 classes, 5 sampels each, 96 numbers each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n",
      "Step 1: loss 0.709532\n",
      "Step 2: loss 0.666055\n",
      "Step 3: loss 0.642281\n",
      "Step 4: loss 0.632864\n",
      "Step 5: loss 0.623283\n",
      "[array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1])]\n",
      "[[0.40263814 0.64289683]\n",
      " [0.3680647  0.6323282 ]\n",
      " [0.36260822 0.63396657]\n",
      " [0.36660993 0.63618577]\n",
      " [0.35929835 0.63285583]\n",
      " [0.36549205 0.63672054]\n",
      " [0.36403674 0.63410556]\n",
      " [0.36428335 0.63634473]\n",
      " [0.37563726 0.6387607 ]\n",
      " [0.36743435 0.63130426]\n",
      " [0.36273825 0.6261605 ]\n",
      " [0.36297527 0.62847614]\n",
      " [0.36467382 0.630107  ]\n",
      " [0.36999714 0.6381821 ]\n",
      " [0.35985848 0.6255747 ]\n",
      " [0.3675767  0.6357739 ]\n",
      " [0.36569425 0.6348956 ]\n",
      " [0.35988078 0.63016343]\n",
      " [0.3605722  0.62999076]\n",
      " [0.36341172 0.6269958 ]\n",
      " [0.3673587  0.6336042 ]\n",
      " [0.37359178 0.6380745 ]\n",
      " [0.3633938  0.6318844 ]\n",
      " [0.36853057 0.6336346 ]\n",
      " [0.3688588  0.63184863]\n",
      " [0.36509168 0.62904763]\n",
      " [0.3671619  0.6364053 ]\n",
      " [0.36265802 0.6279356 ]\n",
      " [0.36500448 0.6315873 ]\n",
      " [0.3592808  0.62920463]\n",
      " [0.37230387 0.63789016]\n",
      " [0.36243787 0.6312907 ]\n",
      " [0.36747748 0.6348128 ]\n",
      " [0.3636144  0.5899016 ]\n",
      " [0.36179832 0.62744385]\n",
      " [0.36278182 0.63181096]\n",
      " [0.36606923 0.63149685]\n",
      " [0.35948634 0.6316278 ]\n",
      " [0.3635084  0.63208365]\n",
      " [0.36494336 0.6339921 ]\n",
      " [0.3607319  0.63297576]\n",
      " [0.3688325  0.6332882 ]\n",
      " [0.36349624 0.6332638 ]\n",
      " [0.37085178 0.6296021 ]\n",
      " [0.36660933 0.62952495]\n",
      " [0.36186787 0.62843   ]\n",
      " [0.36458188 0.62852067]\n",
      " [0.36169654 0.6287759 ]\n",
      " [0.36719096 0.63465375]\n",
      " [0.36299783 0.63466746]\n",
      " [0.36608598 0.6295946 ]\n",
      " [0.36081102 0.6255509 ]\n",
      " [0.3651807  0.63404626]\n",
      " [0.36383566 0.626436  ]\n",
      " [0.36830163 0.63516563]\n",
      " [0.3649688  0.63239115]\n",
      " [0.3636868  0.6300355 ]\n",
      " [0.3700688  0.6298828 ]\n",
      " [0.3672811  0.632766  ]\n",
      " [0.3645878  0.63416606]\n",
      " [0.36252367 0.6314222 ]\n",
      " [0.3638833  0.63267714]\n",
      " [0.36454916 0.63339245]\n",
      " [0.36199751 0.633722  ]\n",
      " [0.3640003  0.6374603 ]\n",
      " [0.3641531  0.62881047]\n",
      " [0.3663931  0.6336494 ]\n",
      " [0.36638165 0.6402684 ]\n",
      " [0.3630726  0.6284274 ]\n",
      " [0.3630139  0.63448995]\n",
      " [0.36556453 0.6323019 ]\n",
      " [0.36463726 0.63006073]\n",
      " [0.36692312 0.6340725 ]\n",
      " [0.3682624  0.63080037]\n",
      " [0.36885872 0.635045  ]\n",
      " [0.36489218 0.626482  ]\n",
      " [0.36458537 0.62998503]\n",
      " [0.36266598 0.63046604]\n",
      " [0.36763123 0.63552094]\n",
      " [0.3636353  0.62515473]\n",
      " [0.36116526 0.6288768 ]\n",
      " [0.36650833 0.63158983]\n",
      " [0.3658729  0.63379896]\n",
      " [0.36342967 0.6277475 ]\n",
      " [0.37157306 0.6405756 ]\n",
      " [0.36467776 0.6319723 ]\n",
      " [0.3651046  0.6327447 ]\n",
      " [0.3602653  0.63001025]\n",
      " [0.3636366  0.63537693]\n",
      " [0.3639506  0.6383843 ]\n",
      " [0.36498138 0.6317857 ]\n",
      " [0.37376645 0.5696306 ]\n",
      " [0.36122423 0.6276409 ]\n",
      " [0.36438444 0.62754536]\n",
      " [0.35963178 0.62616336]\n",
      " [0.36167073 0.62776387]\n",
      " [0.3664523  0.6339073 ]\n",
      " [0.3644828  0.6346204 ]\n",
      " [0.36608702 0.6322533 ]\n",
      " [0.366672   0.6318445 ]\n",
      " [0.36472273 0.6344041 ]\n",
      " [0.36170125 0.6310623 ]\n",
      " [0.37027678 0.6353515 ]\n",
      " [0.3636687  0.6311986 ]\n",
      " [0.3614705  0.63398784]\n",
      " [0.36274704 0.6323141 ]\n",
      " [0.36508793 0.63259727]\n",
      " [0.36799487 0.6361094 ]\n",
      " [0.36690924 0.6382776 ]\n",
      " [0.36316812 0.6296561 ]\n",
      " [0.36152783 0.62584984]\n",
      " [0.35932022 0.6303852 ]\n",
      " [0.36478472 0.62899274]\n",
      " [0.36013436 0.6276751 ]\n",
      " [0.36103755 0.6316087 ]\n",
      " [0.3657979  0.63181895]\n",
      " [0.3686313  0.63473237]\n",
      " [0.36802077 0.63086736]\n",
      " [0.3621291  0.6255207 ]\n",
      " [0.36235043 0.62664294]\n",
      " [0.36395735 0.6312081 ]\n",
      " [0.36768207 0.6389914 ]\n",
      " [0.36626518 0.6341327 ]\n",
      " [0.36746073 0.63697857]\n",
      " [0.36607668 0.6388696 ]\n",
      " [0.3649115  0.634054  ]\n",
      " [0.36280328 0.6271948 ]\n",
      " [0.36630365 0.6359483 ]\n",
      " [0.36088073 0.629459  ]\n",
      " [0.3672824  0.63532317]\n",
      " [0.36259463 0.6347692 ]\n",
      " [0.35977554 0.628947  ]\n",
      " [0.36377868 0.6361657 ]\n",
      " [0.355524   0.63241166]\n",
      " [0.3691728  0.63728976]\n",
      " [0.3630799  0.6297796 ]\n",
      " [0.36369988 0.63266534]\n",
      " [0.36560136 0.63968414]\n",
      " [0.364141   0.63352   ]\n",
      " [0.36801967 0.6325286 ]\n",
      " [0.36927944 0.633848  ]\n",
      " [0.36574033 0.63586587]\n",
      " [0.3618753  0.63478523]\n",
      " [0.36346695 0.6346565 ]\n",
      " [0.3601078  0.6269939 ]\n",
      " [0.3669763  0.6329552 ]\n",
      " [0.35725603 0.6330037 ]\n",
      " [0.35693955 0.6328701 ]\n",
      " [0.3642336  0.63356096]\n",
      " [0.3627256  0.62914735]\n",
      " [0.36134636 0.63407004]\n",
      " [0.36355606 0.6292348 ]\n",
      " [0.36457026 0.63264936]\n",
      " [0.36333564 0.63166124]\n",
      " [0.3657971  0.6366038 ]\n",
      " [0.36700636 0.6384051 ]\n",
      " [0.36140734 0.6293184 ]\n",
      " [0.36087355 0.6363371 ]\n",
      " [0.36283728 0.6295665 ]\n",
      " [0.36462602 0.6344644 ]\n",
      " [0.36491784 0.63144976]\n",
      " [0.36493456 0.63056785]\n",
      " [0.36464682 0.626602  ]\n",
      " [0.3614723  0.6230063 ]\n",
      " [0.36538798 0.6339178 ]\n",
      " [0.3671083  0.63528883]\n",
      " [0.3622536  0.6315025 ]\n",
      " [0.3624985  0.6279434 ]\n",
      " [0.36536136 0.63465923]\n",
      " [0.36558524 0.63219887]\n",
      " [0.36211428 0.6339268 ]\n",
      " [0.36221412 0.62713194]\n",
      " [0.36003035 0.63173425]\n",
      " [0.36015412 0.6333319 ]\n",
      " [0.3618299  0.62789017]\n",
      " [0.36301827 0.6305251 ]\n",
      " [0.3624279  0.62621266]\n",
      " [0.36128467 0.6324704 ]\n",
      " [0.3649622  0.6352147 ]\n",
      " [0.3563741  0.6238959 ]\n",
      " [0.36146706 0.6282637 ]\n",
      " [0.36031905 0.63103193]\n",
      " [0.36446583 0.6288659 ]\n",
      " [0.36841333 0.63517886]\n",
      " [0.3653979  0.63223493]\n",
      " [0.3706443  0.6367989 ]\n",
      " [0.36424792 0.6366881 ]\n",
      " [0.36471653 0.6294347 ]\n",
      " [0.3662139  0.63854396]\n",
      " [0.36623183 0.6348846 ]\n",
      " [0.36026427 0.6287625 ]\n",
      " [0.36347654 0.6325572 ]\n",
      " [0.3643722  0.63270456]\n",
      " [0.3666378  0.6320055 ]\n",
      " [0.36394006 0.6345266 ]\n",
      " [0.37217954 0.63321435]\n",
      " [0.36922762 0.63576126]\n",
      " [0.36977872 0.63396627]\n",
      " [0.36519766 0.6339678 ]\n",
      " [0.36567461 0.63400954]\n",
      " [0.35780188 0.6263214 ]\n",
      " [0.3706536  0.6438468 ]\n",
      " [0.35890487 0.6257612 ]\n",
      " [0.37051982 0.6310543 ]\n",
      " [0.3658751  0.6304966 ]\n",
      " [0.36310866 0.63541657]\n",
      " [0.36081395 0.62469614]\n",
      " [0.36051968 0.6293974 ]\n",
      " [0.36473167 0.63735324]\n",
      " [0.3701561  0.6379901 ]\n",
      " [0.36168084 0.632505  ]\n",
      " [0.36332795 0.6323772 ]\n",
      " [0.3636959  0.63030875]\n",
      " [0.36209905 0.63315165]\n",
      " [0.3639435  0.63798654]\n",
      " [0.36472872 0.6334714 ]\n",
      " [0.36295214 0.6301267 ]\n",
      " [0.36629236 0.6364786 ]\n",
      " [0.36241695 0.6251381 ]\n",
      " [0.36123332 0.62950695]\n",
      " [0.36189717 0.6280936 ]\n",
      " [0.3606185  0.62642705]\n",
      " [0.36510435 0.63779694]\n",
      " [0.37123904 0.6355319 ]\n",
      " [0.3711163  0.6351976 ]\n",
      " [0.36470518 0.6353374 ]\n",
      " [0.36274356 0.6340137 ]\n",
      " [0.3638011  0.6312233 ]\n",
      " [0.36735687 0.63781196]\n",
      " [0.36417386 0.63122827]\n",
      " [0.36739165 0.6336598 ]\n",
      " [0.36162207 0.6278579 ]\n",
      " [0.3647257  0.6338096 ]\n",
      " [0.3601574  0.62639356]\n",
      " [0.3596214  0.6293212 ]\n",
      " [0.36081785 0.62922704]\n",
      " [0.36467114 0.63511235]\n",
      " [0.36541715 0.6318044 ]\n",
      " [0.36617967 0.63323593]\n",
      " [0.36738074 0.6304604 ]\n",
      " [0.36614037 0.6322017 ]\n",
      " [0.36633253 0.6280758 ]\n",
      " [0.36131623 0.6281768 ]\n",
      " [0.36648846 0.6338747 ]\n",
      " [0.3662449  0.6337166 ]\n",
      " [0.36486667 0.63013375]\n",
      " [0.3639848  0.6333116 ]\n",
      " [0.35994008 0.6252192 ]\n",
      " [0.36637855 0.6368639 ]\n",
      " [0.36167568 0.6282601 ]\n",
      " [0.36588803 0.63609844]\n",
      " [0.36548704 0.62966275]\n",
      " [0.35889015 0.62914133]\n",
      " [0.36875916 0.63421595]\n",
      " [0.3677915  0.6333685 ]\n",
      " [0.3646051  0.63372964]\n",
      " [0.3679661  0.63605386]\n",
      " [0.3659109  0.6369105 ]\n",
      " [0.36592388 0.6394301 ]\n",
      " [0.36464214 0.6318445 ]\n",
      " [0.36804542 0.6394229 ]\n",
      " [0.37054548 0.6347403 ]\n",
      " [0.3649293  0.6321127 ]\n",
      " [0.36917686 0.6355471 ]\n",
      " [0.36659783 0.63518435]\n",
      " [0.3656381  0.62916636]\n",
      " [0.364412   0.6337138 ]\n",
      " [0.35839483 0.6268235 ]\n",
      " [0.36994138 0.63848794]\n",
      " [0.36139536 0.63641065]\n",
      " [0.36901155 0.6272544 ]\n",
      " [0.36088052 0.6303821 ]\n",
      " [0.36228052 0.6298119 ]\n",
      " [0.36619648 0.634962  ]\n",
      " [0.3650983  0.6309679 ]\n",
      " [0.36719057 0.6289953 ]\n",
      " [0.36979854 0.6377024 ]\n",
      " [0.3609462  0.6333462 ]\n",
      " [0.36283642 0.6342668 ]\n",
      " [0.36123228 0.63153636]\n",
      " [0.36126146 0.6261964 ]\n",
      " [0.37058565 0.63460946]\n",
      " [0.36176312 0.6327365 ]\n",
      " [0.36087093 0.62899095]\n",
      " [0.36886254 0.637181  ]\n",
      " [0.36255154 0.63102   ]\n",
      " [0.3665418  0.637835  ]\n",
      " [0.35951328 0.6264817 ]\n",
      " [0.35980767 0.627672  ]\n",
      " [0.36481133 0.63216066]\n",
      " [0.36540008 0.633268  ]\n",
      " [0.3733962  0.63778436]\n",
      " [0.3660647  0.6337377 ]\n",
      " [0.36663723 0.6288285 ]\n",
      " [0.3627359  0.626073  ]\n",
      " [0.36473384 0.63316226]\n",
      " [0.36123127 0.6346687 ]\n",
      " [0.36442426 0.63347185]\n",
      " [0.36914048 0.6370584 ]\n",
      " [0.36707243 0.6311086 ]\n",
      " [0.36864808 0.63091135]\n",
      " [0.36835217 0.637638  ]\n",
      " [0.3835215  0.58762705]\n",
      " [0.38346168 0.62663287]\n",
      " [0.3662982  0.63646543]\n",
      " [0.37336585 0.6376782 ]\n",
      " [0.36946332 0.638017  ]\n",
      " [0.36428866 0.6264618 ]\n",
      " [0.36205423 0.63035804]\n",
      " [0.3644369  0.63526803]\n",
      " [0.3650671  0.63209856]\n",
      " [0.3683131  0.6383726 ]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "_NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "def train(get_examples, _NUM_BATCHES = 50):\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        pred = None\n",
    "        # Define VGGish.\n",
    "        embeddings = vggish_slim.define_vggish_slim(True) # Do we train VGG-ish?\n",
    "\n",
    "        # Define a shallow classification model and associated training ops on top\n",
    "        # of VGGish.\n",
    "        with tf.variable_scope('mymodel'):\n",
    "            # Add a fully connected layer with 100 units.\n",
    "            num_units = 100\n",
    "            fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "            # classifiers, one per class. This allows for multi-class tasks.\n",
    "            logits = slim.fully_connected(\n",
    "              fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
    "            pred = tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "            # Add training ops.\n",
    "            with tf.variable_scope('train'):\n",
    "                global_step = tf.Variable(\n",
    "                    0, name='global_step', trainable=False,\n",
    "                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                 tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "            # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "            # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "            labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
    "\n",
    "            # Cross-entropy label loss.\n",
    "            xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=logits, labels=labels, name='xent')\n",
    "            loss = tf.reduce_mean(xent, name='loss_op')\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=vggish_params.LEARNING_RATE,\n",
    "                epsilon=vggish_params.ADAM_EPSILON)\n",
    "            optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "\n",
    "        # Initialize all variables in the model, and then load the pre-trained\n",
    "        # VGGish checkpoint.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        #for op in tf.get_default_graph().get_operations():\n",
    "            #print(str(op.name))\n",
    "\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/labels:0')\n",
    "        #labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')    \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/loss_op:0')\n",
    "        train_op = sess.graph.get_operation_by_name('mymodel/train_op')\n",
    "\n",
    "        # The training loop.\n",
    "        for _ in range(_NUM_BATCHES):\n",
    "          (features, labels) = get_examples(shuf=True)\n",
    "          [num_steps, loss, _] = sess.run(\n",
    "              [global_step_tensor, loss_tensor, train_op],\n",
    "              feed_dict={features_tensor: features, labels_tensor: labels})\n",
    "          print('Step %d: loss %g' % (num_steps, loss))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './model/model_%s' % (num))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #pproc = vggish_postprocess.Postprocessor('./vggish_pca_params.npz')\n",
    "        #print(vggish_params)\n",
    "        # Define the model in inference mode, load the checkpoint, and\n",
    "        # locate input and output tensors.\n",
    "        #vggish_slim.define_vggish_slim(training=False)\n",
    "        #vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        prediction=tf.argmax(logits,1)\n",
    "        (features, labels) = get_examples(shuf=False)\n",
    "        #print(labels)\n",
    "        #best = sess.run([prediction],feed_dict)\n",
    "        # Run inference and postprocessing.\n",
    "        embedding_batch = sess.run(pred,\n",
    "                                     feed_dict={features_tensor: features})\n",
    "        #print(embedding_batch.shape)\n",
    "        #print(embedding_batch)\n",
    "        #preds = sess.run(tf.nn.softmax(embedding_batch[0], 1))\n",
    "        #print(preds)\n",
    "        #preds=tf.argmax(embedding_batch,1)\n",
    "        #print(preds.eval(feed_dict={features_tensor: features}))\n",
    "        return embedding_batch \n",
    "    \n",
    "def getSavedSamples(shuf = True):\n",
    "    features = np.load('checkpoints/features_%s.npy' % (num))\n",
    "    labels = np.load('checkpoints/labels_%s.npy' % (num))\n",
    "    \n",
    "    labeled_examples = list(zip(features, labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "preds = train(getSavedSamples, 5)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    print(preds)\n",
    "    print(sess.run(tf.argmax(input=preds, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_postprocess\n",
    "\n",
    "(features, labels) = _get_examples_batch2()\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    pproc = vggish_postprocess.Postprocessor('./vggish_pca_params.npz')\n",
    "    # Define the model in inference mode, load the checkpoint, and\n",
    "    # locate input and output tensors.\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt')\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    # Run inference and postprocessing.\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: features})\n",
    "    print(embedding_batch)\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "    print(postprocessed_batch)\n",
    "\n",
    "    # Write the postprocessed embeddings as a SequenceExample, in a similar\n",
    "    # format as the features released in AudioSet. Each row of the batch of\n",
    "    # embeddings corresponds to roughly a second of audio (96 10ms frames), and\n",
    "    # the rows are written as a sequence of bytes-valued features, where each\n",
    "    # feature value contains the 128 bytes of the whitened quantized embedding.\n",
    "    seq_example = tf.train.SequenceExample(\n",
    "        feature_lists=tf.train.FeatureLists(\n",
    "            feature_list={\n",
    "                vggish_params.AUDIO_EMBEDDING_FEATURE_NAME:\n",
    "                    tf.train.FeatureList(\n",
    "                        feature=[\n",
    "                            tf.train.Feature(\n",
    "                                bytes_list=tf.train.BytesList(\n",
    "                                    value=[embedding.tobytes()]))\n",
    "                            for embedding in postprocessed_batch\n",
    "                        ]\n",
    "                    )\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print(seq_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = [\n",
    "    '%s/0.000000_5.000000.wav' % (directory),\n",
    "    '%s/5.000000_10.000000.wav' % (directory),\n",
    "    '%s/10.000000_15.000000.wav' % (directory),\n",
    "    #'%s/15.000000_20.000000.wav' % (directory),\n",
    "    #'%s/20.000000_25.000000.wav' % (directory),    \n",
    "]\n",
    "def _get_examples_batch2(shuf = True):\n",
    "    \"\"\"Returns a shuffled batch of examples of all audio classes.\n",
    "\n",
    "    Note that this is just a toy function because this is a simple demo intended\n",
    "    to illustrate how the training code might work.\n",
    "\n",
    "    Returns:\n",
    "    a tuple (features, labels) where features is a NumPy array of shape\n",
    "    [batch_size, num_frames, num_bands] where the batch_size is variable and\n",
    "    each row is a log mel spectrogram patch of shape [num_frames, num_bands]\n",
    "    suitable for feeding VGGish, while labels is a NumPy array of shape\n",
    "    [batch_size, num_classes] where each row is a multi-hot label vector that\n",
    "    provides the labels for corresponding rows in features.\n",
    "    \"\"\"\n",
    "    # Make a waveform for each class.\n",
    "    num_seconds = 5 * (len(files) * 2)\n",
    "    \n",
    "    sr = 44100  # Sampling rate.\n",
    "    t = np.linspace(0, num_seconds, int(num_seconds * sr))  # Time axis.\n",
    "    \n",
    "    # White noise.\n",
    "    noise = np.random.normal(-1, 1, size=t.shape)\n",
    "\n",
    "    sample = np.array([])\n",
    "    for file in files:\n",
    "        sample = np.append(sample, AudioSegment.from_file(file).get_array_of_samples())\n",
    "    #sample = np.array(samples)\n",
    "    \n",
    "    #print(first_sample.shape)\n",
    "    #sine_examples = []\n",
    "    #for file in files:\n",
    "    #    sine_examples.append(AudioSegment.from_file(files[0]).get_array_of_samples()[:96])\n",
    "\n",
    "    #sine_examples = np.array(sine_examples)\n",
    "    # Random sine wave.\n",
    "\n",
    "    # Make examples of each signal and corresponding labels.\n",
    "    # Sine is class index 0, Const class index 1, Noise class index 2.\n",
    "    sine_examples = vggish_input.waveform_to_examples(sample, sr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #freq = np.random.uniform(100, 1000)\n",
    "    #sine = np.sin(2 * np.pi * freq * t)\n",
    "    #magnitude = np.random.uniform(-1, 1)\n",
    "    #print(magnitude)\n",
    "    #const = -1 * t\n",
    "    # Random constant signal.\n",
    "    #magnitude2 = np.random.uniform(-1, 1)\n",
    "    #print(magnitude2)\n",
    "    #const2 = 0 * t\n",
    "    # White noise.\n",
    "    noise = np.random.normal(-1, 1, size=t.shape)\n",
    "\n",
    "    # Make examples of each signal and corresponding labels.\n",
    "    # Sine is class index 0, Const class index 1, Noise class index 2.\n",
    "    #sine_examples = vggish_input.waveform_to_examples(const, sr)\n",
    "    sine_labels = np.array([[1, 0]] * sine_examples.shape[0])\n",
    "    #sine_examples2 = vggish_input.waveform_to_examples(const2, sr)\n",
    "    #sine_labels2 = np.array([[0, 1]] * sine_examples.shape[0])    \n",
    "    #const_examples = vggish_input.waveform_to_examples(const, sr)\n",
    "    #const_labels = np.array([[0, 1, 0]] * const_examples.shape[0])\n",
    "    noise_examples = vggish_input.waveform_to_examples(noise, sr)\n",
    "    noise_labels = np.array([[0, 1]] * noise_examples.shape[0])\n",
    "\n",
    "    #print(sine_examples[0])\n",
    "    #print('---')\n",
    "    #print(sine_examples2[0])\n",
    "\n",
    "    #print(noise_examples.shape)\n",
    "\n",
    "    # Shuffle (example, label) pairs across all classes.\n",
    "    all_examples = np.concatenate((sine_examples, noise_examples))\n",
    "    all_labels = np.concatenate((sine_labels, noise_labels))\n",
    "    labeled_examples = list(zip(all_examples, all_labels))\n",
    "    if shuf:\n",
    "        shuffle(labeled_examples)\n",
    "\n",
    "    # Separate and return the features and labels.\n",
    "    features = [example for (example, _) in labeled_examples]\n",
    "    labels = [label for (_, label) in labeled_examples]\n",
    "    return (features, labels)\n",
    "\n",
    "(features, labels) = _get_examples_batch2()\n",
    "print(len(features))\n",
    "# returns 3 classes, 5 sampels each, 96 numbers each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python vggish_train_demo.py --num_batches 50 --train_vggish=False --checkpoint './vggish_model.ckpt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
